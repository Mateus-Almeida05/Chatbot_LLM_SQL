# -*- coding: utf-8 -*-
"""Chatbot com SQLite + LangChain + Modelos Hugging Face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNu29WP66fWFiA_3sAjue8mRMr1napB9

##**Chatbot com SQLite + LangChain + Modelos Hugging Face**

**1. Prepara√ß√£o do Ambiente**
"""

#Instala√ß√£o das depend√™ncias
!pip install --upgrade --quiet transformers accelerate bitsandbytes sentencepiece langchain langchain-community langchain-experimental

#instala√ß√£o das bibliotecas
import sqlite3
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.utilities import SQLDatabase
from langchain_experimental.sql import SQLDatabaseChain
from huggingface_hub import login

"""**2. Cria√ß√£o do Banco de Dados e Inser√ß√£o de Clientes**

Nesta etapa foi criado um banco de dados SQLite chamado example.db, contendo a tabela clientes. A tabela foi definida com os seguintes campos:

  - id (chave prim√°ria, inteiro)

  - nome (texto)

  - email (texto)

  - cidade (texto)

Antes da inser√ß√£o, a tabela √© limpa para evitar duplica√ß√£o de registros. Em seguida, foram adicionados tr√™s clientes fict√≠cios:

  - Ana Silva ‚Äì S√£o Paulo

  - Bruno Souza ‚Äì Rio de Janeiro

  - Carla Mendes ‚Äì Belo Horizonte

Por fim, os dados foram consultados e exibidos em um DataFrame do pandas, confirmando que a tabela foi criada e populada corretamente.
"""

# Criar banco e tabela de clientes
conn = sqlite3.connect("example.db")
cursor = conn.cursor()

cursor.execute("""
CREATE TABLE IF NOT EXISTS clientes (
    id INTEGER PRIMARY KEY,
    nome TEXT,
    email TEXT,
    cidade TEXT
)
""")

# Inserir dados fict√≠cios
cursor.execute("DELETE FROM clientes")  # limpa antes para evitar duplica√ß√£o
cursor.execute("INSERT INTO clientes (nome, email, cidade) VALUES ('Ana Silva', 'ana@email.com', 'S√£o Paulo')")
cursor.execute("INSERT INTO clientes (nome, email, cidade) VALUES ('Bruno Souza', 'bruno@email.com', 'Rio de Janeiro')")
cursor.execute("INSERT INTO clientes (nome, email, cidade) VALUES ('Carla Mendes', 'carla@email.com', 'Belo Horizonte')")

conn.commit()
conn.close()

# Visualizar dados
conn = sqlite3.connect("example.db")
df = pd.read_sql_query("SELECT * FROM clientes", conn)
conn.close()
df

"""**3. Autentica√ß√£o no Hugging Face**

Para utilizar modelos hospedados na plataforma Hugging Face, √© necess√°rio autenticar-se com um token de acesso pessoal.

No c√≥digo, o token deve ser colado na vari√°vel HF_TOKEN entre aspas.

Ap√≥s executar o comando login(HF_TOKEN), a autentica√ß√£o ser√° conclu√≠da e ser√° poss√≠vel carregar e utilizar modelos da Hugging Face no projeto.

Observa√ß√£o: o token pode ser obtido na sua conta do Hugging Face, na se√ß√£o Access Tokens.
"""

#Cole seu token do Hugging Face aqui (entre aspas)
HF_TOKEN = "hf.."

login(HF_TOKEN)
print("üîë Login no Hugging Face realizado com sucesso!")

"""**4. Integra√ß√£o com LangChain**

Antes de iniciarmos a implementa√ß√£o do chatbot, foi necess√°rio carregar e configurar o modelo de linguagem que far√° a interpreta√ß√£o das perguntas em linguagem natural e a gera√ß√£o das respostas. Para isso, utilizamos o modelo Mistral-7B-Instruct, dispon√≠vel na Hugging Face, que foi carregado com otimiza√ß√µes de mem√≥ria (quantiza√ß√£o em 4 bits e offload autom√°tico para CPU/GPU).

Em seguida, criamos um pipeline de gera√ß√£o de texto ajustado para produzir sa√≠das curtas, consistentes e determin√≠sticas, ideais para tarefas como gera√ß√£o de consultas SQL e respostas objetivas. Por fim, integramos esse pipeline ao LangChain por meio da classe HuggingFacePipeline, tornando o modelo pronto para ser utilizado dentro da aplica√ß√£o de chatbot.
"""

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,   # reduz uso de mem√≥ria
    device_map="auto",
    llm_int8_enable_fp32_cpu_offload=True,
    trust_remote_code=True
)

# Criar pipeline de gera√ß√£o de texto
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=50,     # suficiente para SQL + resposta
    temperature=0.1,        # mais consistente
    do_sample=False,        # determin√≠stico
    return_full_text=False
)


# Conectar modelo ao LangChain
llm = HuggingFacePipeline(pipeline=generator)
print("‚úÖ Modelo Mistral-7B-Instruct carregado com sucesso!")

"""**5. Integra√ß√£o entre Banco de Dados SQLite e LangChain**

Nesta etapa, realizamos a integra√ß√£o entre o banco de dados SQLite e o LangChain. Primeiro, foi estabelecida a conex√£o com o banco example.db, onde est√° armazenada a tabela de clientes. Em seguida, criamos um pipeline que combina o modelo de linguagem (LLM) com o banco de dados, permitindo que o chatbot interprete perguntas em linguagem natural, traduza essas perguntas em consultas SQL e retorne a resposta diretamente ao usu√°rio.

Com isso, o chatbot est√° pronto para responder perguntas sobre os clientes cadastrados no banco, unindo o poder dos modelos de linguagem da Hugging Face com a manipula√ß√£o estruturada de dados via SQLite.
"""

# Conectar SQLite ao LangChain
db = SQLDatabase.from_uri("sqlite:///example.db")

# Criar pipeline: modelo + banco
db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=False, return_direct=True)

print("ü§ñ Chatbot pronto para responder perguntas!")

"""**6. Teste**

Nesta etapa, foi implementada a fun√ß√£o perguntar(), respons√°vel por receber uma quest√£o em linguagem natural, executar a consulta no banco de dados SQLite e devolver a resposta ao usu√°rio em um formato claro e amig√°vel.

O fluxo funciona da seguinte maneira:

  - O LangChain interpreta a pergunta e a transforma em uma consulta SQL para buscar a resposta no banco.

  - Se o retorno vier em formato estruturado (lista, tupla ou dicion√°rio), o c√≥digo faz uma formata√ß√£o pr√©via para simplificar os dados.

  - Em seguida, √© utilizado novamente o modelo de linguagem (LLM) para reformular a resposta bruta em portugu√™s natural, tornando a intera√ß√£o mais intuitiva.

Por fim, foi criado um loop interativo no terminal, onde o usu√°rio pode digitar suas perguntas em portugu√™s e receber as respostas do chatbot. O comando "sair" encerra a sess√£o.

Esse passo conclui a integra√ß√£o entre:

  - Modelo Hugging Face (Mistral-7B-Instruct) para compreens√£o e gera√ß√£o de linguagem natural.

  - LangChain para orquestra√ß√£o entre linguagem natural e SQL.

  - SQLite como fonte de dados persistente.

O resultado √© um chatbot capaz de responder perguntas como:

‚ÄúQuais clientes moram em S√£o Paulo?‚Äù

‚ÄúQual √© o e-mail de Carla Mendes?‚Äù
"""

def perguntar(pergunta):
    # Executa a query normalmente
    resposta = db_chain.invoke(pergunta)

    # Se vier dicion√°rio, pega s√≥ o campo 'result'
    if isinstance(resposta, dict) and "result" in resposta:
        resposta = resposta["result"]

    # Pr√©-formatar resultado em texto simples
    if isinstance(resposta, list):
        frases = []
        for r in resposta:
            if isinstance(r, tuple):
                frases.append(", ".join(map(str, r)))
            else:
                frases.append(str(r))
        resposta = "; ".join(frases)
    elif isinstance(resposta, (tuple, list)):
        resposta = ", ".join(map(str, resposta))

    # Agora pedir ao modelo para deixar natural
    prompt = f"""
    Reformule a seguinte resposta do banco de dados em portugu√™s, de forma clara e amig√°vel:

    Pergunta: {pergunta}
    Resposta bruta: {resposta}

    Resposta natural:
    """
    resposta_final = llm(prompt)

    return f"üìå Pergunta: {pergunta}\nü§ñ Resposta: {resposta_final}"

    print("ü§ñ Chatbot iniciado! Digite sua pergunta ou 'sair' para encerrar.\n")

# --- Loop interativo ---
print("ü§ñ Chatbot conectado ao banco de dados!")
print("Digite sua pergunta em portugu√™s ou 'sair' para encerrar.\n")

while True:
    pergunta = input("üßë Voc√™: ")
    if pergunta.strip().lower() in ["sair", "exit", "quit"]:
        print("ü§ñ Bot: At√© logo!")
        break

    resposta = perguntar(pergunta)
    print(f"ü§ñ Bot: {resposta}\n")